import os
import pandas as pd
import re
from pathlib import Path

# ==== CONFIG ====
INPUT_CSV = "/Users/user/Documents/AI/output/TEST/merged_output.csv"
OUTPUT_DIR = "/Users/user/Documents/AI/output/TEST"
OUTPUT_FILE = "final_output.csv"

# Column names to check for null/empty values
CHECK_COLUMNS = ["Task evidence Q and A", "Task evidence Q and A Reason"]
URL_COLUMN = "Task Evidence"  # Column to extract URLs from

# ==== STEP 1: Create output directory ====
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"ðŸ“ Output directory: {OUTPUT_DIR}\n")

# ==== STEP 2: Read CSV ====
print(f"ðŸ“– Reading CSV: {INPUT_CSV}")
try:
    df = pd.read_csv(INPUT_CSV)
    print(f"âœ… Loaded {len(df)} rows, {len(df.columns)} columns\n")
except Exception as e:
    print(f"âŒ Error reading CSV: {e}")
    exit(1)

# ==== STEP 3: Verify columns exist ====
missing_cols = []
for col in CHECK_COLUMNS + [URL_COLUMN]:
    if col not in df.columns:
        missing_cols.append(col)

if missing_cols:
    print(f"âŒ Missing columns in CSV: {', '.join(missing_cols)}")
    print(f"\nðŸ“‹ Available columns:")
    for col in df.columns:
        print(f"   â€¢ {col}")
    exit(1)

# ==== STEP 4: Identify rows with null/empty values ====
print(f"ðŸ” Checking for null/empty values in:")
print(f"   â€¢ {CHECK_COLUMNS[0]}")
print(f"   â€¢ {CHECK_COLUMNS[1]}\n")

# Create mask for rows with null or empty values in either column
mask = df[CHECK_COLUMNS[0]].isna() | df[CHECK_COLUMNS[1]].isna() | \
       (df[CHECK_COLUMNS[0]].astype(str).str.strip() == '') | \
       (df[CHECK_COLUMNS[1]].astype(str).str.strip() == '')

rows_with_nulls = df[mask].copy()
rows_to_keep = df[~mask].copy()

null_count = len(rows_with_nulls)
print(f"ðŸ“Š Found {null_count} rows with null/empty values")
print(f"âœ… {len(rows_to_keep)} rows are valid and will be kept\n")

# ==== STEP 5: Extract URLs from null rows ====
extracted_urls = []

if null_count > 0:
    print(f"ðŸ”— Extracting URLs from '{URL_COLUMN}' column:\n")
    print("=" * 80)
    
    for idx, row in rows_with_nulls.iterrows():
        url_value = row[URL_COLUMN]
        
        # Extract URL using regex (handles various formats)
        if pd.notna(url_value):
            url_str = str(url_value)
            # Find URLs in the text
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
            found_urls = re.findall(url_pattern, url_str)
            
            if found_urls:
                for url in found_urls:
                    extracted_urls.append(url)
                    print(f"   â€¢ {url}")
            else:
                # If no URL pattern found, print the raw value
                extracted_urls.append(url_str)
                print(f"   â€¢ {url_str}")
        else:
            print(f"   â€¢ [No URL - Cell is empty]")
    
    print("=" * 80)
    print(f"\nðŸ“ Total URLs extracted: {len(extracted_urls)}\n")
else:
    print("âœ… No null/empty rows found - nothing to extract\n")

# ==== STEP 6: Save cleaned CSV ====
output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
rows_to_keep.to_csv(output_path, index=False)
output_size = os.path.getsize(output_path) / 1024

print(f"ðŸ’¾ Cleaned CSV saved to: {output_path}")
print(f"   â€¢ Size: {output_size:.2f} KB\n")

# ==== STEP 7: Generate Report ====
print("=" * 80)
print(" CLEANING REPORT ".center(80, "="))
print("=" * 80)

print(f"\nðŸ“ INPUT:")
print(f"   â€¢ File: {INPUT_CSV}")
print(f"   â€¢ Original rows: {len(df)}")

print(f"\nðŸ” CHECKED COLUMNS:")
print(f"   â€¢ {CHECK_COLUMNS[0]}")
print(f"   â€¢ {CHECK_COLUMNS[1]}")

print(f"\nðŸ“Š RESULTS:")
print(f"   â€¢ Rows with null/empty values: {null_count}")
print(f"   â€¢ Rows removed: {null_count}")
print(f"   â€¢ Rows kept: {len(rows_to_keep)}")
print(f"   â€¢ URLs extracted: {len(extracted_urls)}")

print(f"\nðŸ’¾ OUTPUT:")
print(f"   â€¢ File: {output_path}")
print(f"   â€¢ Final row count: {len(rows_to_keep)}")
print(f"   â€¢ Size: {output_size:.2f} KB")

if len(extracted_urls) > 0:
    print(f"\nðŸ”— EXTRACTED URLS ({len(extracted_urls)}):")
    print("=" * 80)
    for i, url in enumerate(extracted_urls, 1):
        print(f"{i}. {url}")
    print("=" * 80)

print("\n" + "=" * 80)
print(" CLEANING COMPLETED SUCCESSFULLY ".center(80, "="))
print("=" * 80 + "\n")